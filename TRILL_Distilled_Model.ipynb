{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from scipy import signal\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers import LSTM, Input, Flatten, Embedding, Conv1D, Conv2D, Dropout, MaxPooling2D, Bidirectional\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llcTwWpUEHPA",
        "outputId": "c95253b2-57d4-411c-ffb5-0f8b156a12d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "assert tf.executing_eagerly()\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import pickle\n",
        "emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
        "data_path = \"/content/drive/MyDrive/Data/\"\n",
        "sessions = ['Session1','Session2','Session3','Session4','Session5']\n",
        "\n",
        "with open(data_path +'/data_collected2.pickle', 'rb') as handle:\n",
        "    data2 = pickle.load(handle)\n",
        "\n",
        "# read output label\n",
        "Y=[]\n",
        "for ses_mod in data2:\n",
        "    Y.append(ses_mod['emotion'])\n",
        "Y = label_binarize(y=Y, classes=emotions_used)\n",
        "Y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLC05SBvEHhG",
        "outputId": "f93449d9-935f-4165-c028-b30ac1d9b487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       [0, 0, 1, 0],\n",
              "       ...,\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0],\n",
              "       [1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wav_as_float_or_int16 = data2[0]['signal']\n",
        "# module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3')\n",
        "# emb_dict = module(samples=wav_as_float_or_int16, sample_rate=16000)\n",
        "# emb_layer19 = emb_dict['layer19']\n",
        "# print(emb_layer19.shape)\n",
        "# trill44.append(emb_layer19)"
      ],
      "metadata": {
        "id": "dofz23z4eYhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trill_features = []\n",
        "for ses_mod in data2:\n",
        "  wav_as_float_or_int16 = ses_mod['signal']\n",
        "  module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3')\n",
        "  emb = module(samples=wav_as_float_or_int16, sample_rate=16000)['embedding']\n",
        "  trill_features.append(np.array(emb))\n",
        "\n",
        "\n",
        "trill_features = np.array(trill_features)\n",
        "trill_features.shape\n",
        "np.save('trill_features.npy', trill_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEXU8HqMZbY-",
        "outputId": "f379586e-7791-4aca-e68d-a5dd33fec74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-1c22756a697b>:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  trill_features = np.array(trill_features)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = np.load('/content/drive/MyDrive/Data/trill_features.npy',allow_pickle=True)\n",
        "feature_graph=[]\n",
        "for i in range(0,len(X1)):\n",
        "  feature_graph.append(X1[i].shape[0])\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "fig, ax = plt.subplots(figsize =(10,7))\n",
        "ax.hist(feature_graph, bins = 10)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "tl4dhcC88OvC",
        "outputId": "b69be25d-f34a-4119-a638-8c1e6bdadce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from scipy.io.wavfile import read\n",
        "# a = read(\"/content/460598__mirwanda__ivr-silence-16-bit-mono-16-khz.wav\")\n",
        "\n",
        "# sil= np.array(a[1],dtype='int16')\n",
        "\n",
        "\n",
        "# silence_wavfile =[]\n",
        "\n",
        "# module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3')\n",
        "# emb = module(samples=sil, sample_rate=16000)['embedding']\n",
        "# print(emb.shape)\n",
        "# silence_wavfile.append(np.array(emb))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ks4QyI0QwPW",
        "outputId": "ce4eb1c0-9682-42b0-a108-6869048a2a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "249792"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "silence_duration = 1\n",
        "sample_rate = 16000\n",
        "\n",
        "# Calculate the number of samples needed for the specified duration\n",
        "num_samples = int(silence_duration * sample_rate)\n",
        "\n",
        "# Generate an array of zeros representing silence\n",
        "silence_array = np.zeros(num_samples, dtype=np.int16)\n",
        "print(silence_array)\n",
        "\n",
        "silence_cg =[]\n",
        "\n",
        "module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3')\n",
        "emb = module(samples=silence_array, sample_rate=16000)['embedding']\n",
        "print(emb.shape)\n",
        "silence_cg.append(np.array(emb))\n",
        "silence_trill_values = silence_cg[0][0]\n",
        "silence_trill_values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3jlPIWCDENy",
        "outputId": "968ac7b2-dd45-44f2-c7f5-56d9ea82b8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 0]\n",
            "(1, 2048)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.23181325, -0.01003219,  0.00422064, ..., -0.08759604,\n",
              "        0.03211117, -0.03920655], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trill_features_normalized=[]\n",
        "\n",
        "thres=20\n",
        "for ses_mod in data2:\n",
        "  wav_as_float_or_int16 = ses_mod['signal']\n",
        "  module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3')\n",
        "  emb = module(samples=wav_as_float_or_int16, sample_rate=16000)['embedding']\n",
        "  if(emb.shape[0]<thres):\n",
        "    while(emb.shape[0]<thres):\n",
        "      emb=np.vstack((emb,silence_trill_values))\n",
        "  elif(emb.shape[0]>thres):\n",
        "      emb=emb[:thres]\n",
        "  trill_features_normalized.append(np.array(emb))\n",
        "np.save('trill_features_normalized_thres20.npy',trill_features_normalized)"
      ],
      "metadata": {
        "id": "bTNyvPhhhzA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.load('trill_features_normalized_thres20.npy')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCz1AyCAquch",
        "outputId": "23616567-ec62-4746-d82b-79333f332c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4936, 20, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 1"
      ],
      "metadata": {
        "id": "-nhdt5AcK-Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def speech_model1():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(512, return_sequences=True, input_shape=(20,2048)))\n",
        "    model.add(LSTM(256, return_sequences=False))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(4))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model1 = speech_model1()\n",
        "model1.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiSUVLtfLC-s",
        "outputId": "717eeac7-2cb9-4f1c-90d0-ed8b53a808ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 20, 512)           5244928   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               787456    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               131584    \n",
            "                                                                 \n",
            " activation (Activation)     (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 4)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,166,020\n",
            "Trainable params: 6,166,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\"best_model1.hdf5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto', save_freq='epoch')\n",
        "hist1 = model1.fit(X_train, Y_train, batch_size=32, epochs=50, verbose=1, shuffle = False,callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUJtaGhHLDQi",
        "outputId": "1ca323e9-8d60-43c4-8383-f84802c86a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.3800 - accuracy: 0.3248\n",
            "Epoch 1: loss improved from inf to 1.38001, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 98s 896ms/step - loss: 1.3800 - accuracy: 0.3248\n",
            "Epoch 2/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.3264 - accuracy: 0.3765\n",
            "Epoch 2: loss improved from 1.38001 to 1.32642, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 875ms/step - loss: 1.3264 - accuracy: 0.3765\n",
            "Epoch 3/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.2037 - accuracy: 0.4348\n",
            "Epoch 3: loss improved from 1.32642 to 1.20367, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 877ms/step - loss: 1.2037 - accuracy: 0.4348\n",
            "Epoch 4/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.1522 - accuracy: 0.4602\n",
            "Epoch 4: loss improved from 1.20367 to 1.15224, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 871ms/step - loss: 1.1522 - accuracy: 0.4602\n",
            "Epoch 5/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.1132 - accuracy: 0.4871\n",
            "Epoch 5: loss improved from 1.15224 to 1.11322, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 878ms/step - loss: 1.1132 - accuracy: 0.4871\n",
            "Epoch 6/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0843 - accuracy: 0.5068\n",
            "Epoch 6: loss improved from 1.11322 to 1.08431, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 873ms/step - loss: 1.0843 - accuracy: 0.5068\n",
            "Epoch 7/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0643 - accuracy: 0.5159\n",
            "Epoch 7: loss improved from 1.08431 to 1.06430, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 871ms/step - loss: 1.0643 - accuracy: 0.5159\n",
            "Epoch 8/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0460 - accuracy: 0.5225\n",
            "Epoch 8: loss improved from 1.06430 to 1.04604, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 93s 891ms/step - loss: 1.0460 - accuracy: 0.5225\n",
            "Epoch 9/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0249 - accuracy: 0.5252\n",
            "Epoch 9: loss improved from 1.04604 to 1.02493, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 884ms/step - loss: 1.0249 - accuracy: 0.5252\n",
            "Epoch 10/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0056 - accuracy: 0.5422\n",
            "Epoch 10: loss improved from 1.02493 to 1.00563, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 880ms/step - loss: 1.0056 - accuracy: 0.5422\n",
            "Epoch 11/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9995 - accuracy: 0.5404\n",
            "Epoch 11: loss improved from 1.00563 to 0.99954, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 884ms/step - loss: 0.9995 - accuracy: 0.5404\n",
            "Epoch 12/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9813 - accuracy: 0.5522\n",
            "Epoch 12: loss improved from 0.99954 to 0.98132, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 93s 892ms/step - loss: 0.9813 - accuracy: 0.5522\n",
            "Epoch 13/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9654 - accuracy: 0.5685\n",
            "Epoch 13: loss improved from 0.98132 to 0.96545, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 874ms/step - loss: 0.9654 - accuracy: 0.5685\n",
            "Epoch 14/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9530 - accuracy: 0.5733\n",
            "Epoch 14: loss improved from 0.96545 to 0.95303, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 889ms/step - loss: 0.9530 - accuracy: 0.5733\n",
            "Epoch 15/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9394 - accuracy: 0.5836\n",
            "Epoch 15: loss improved from 0.95303 to 0.93944, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 93s 886ms/step - loss: 0.9394 - accuracy: 0.5836\n",
            "Epoch 16/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9226 - accuracy: 0.5891\n",
            "Epoch 16: loss improved from 0.93944 to 0.92262, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 877ms/step - loss: 0.9226 - accuracy: 0.5891\n",
            "Epoch 17/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9039 - accuracy: 0.6002\n",
            "Epoch 17: loss improved from 0.92262 to 0.90385, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 886ms/step - loss: 0.9039 - accuracy: 0.6002\n",
            "Epoch 18/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8948 - accuracy: 0.6033\n",
            "Epoch 18: loss improved from 0.90385 to 0.89479, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 880ms/step - loss: 0.8948 - accuracy: 0.6033\n",
            "Epoch 19/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8904 - accuracy: 0.6093\n",
            "Epoch 19: loss improved from 0.89479 to 0.89043, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 881ms/step - loss: 0.8904 - accuracy: 0.6093\n",
            "Epoch 20/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8598 - accuracy: 0.6238\n",
            "Epoch 20: loss improved from 0.89043 to 0.85985, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 875ms/step - loss: 0.8598 - accuracy: 0.6238\n",
            "Epoch 21/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8482 - accuracy: 0.6350\n",
            "Epoch 21: loss improved from 0.85985 to 0.84816, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 884ms/step - loss: 0.8482 - accuracy: 0.6350\n",
            "Epoch 22/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8245 - accuracy: 0.6408\n",
            "Epoch 22: loss improved from 0.84816 to 0.82452, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 93s 889ms/step - loss: 0.8245 - accuracy: 0.6408\n",
            "Epoch 23/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8104 - accuracy: 0.6577\n",
            "Epoch 23: loss improved from 0.82452 to 0.81035, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 887ms/step - loss: 0.8104 - accuracy: 0.6577\n",
            "Epoch 24/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7891 - accuracy: 0.6619\n",
            "Epoch 24: loss improved from 0.81035 to 0.78907, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 93s 893ms/step - loss: 0.7891 - accuracy: 0.6619\n",
            "Epoch 25/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7694 - accuracy: 0.6786\n",
            "Epoch 25: loss improved from 0.78907 to 0.76940, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 877ms/step - loss: 0.7694 - accuracy: 0.6786\n",
            "Epoch 26/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7429 - accuracy: 0.6801\n",
            "Epoch 26: loss improved from 0.76940 to 0.74286, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 864ms/step - loss: 0.7429 - accuracy: 0.6801\n",
            "Epoch 27/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7153 - accuracy: 0.6994\n",
            "Epoch 27: loss improved from 0.74286 to 0.71533, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 861ms/step - loss: 0.7153 - accuracy: 0.6994\n",
            "Epoch 28/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6969 - accuracy: 0.7109\n",
            "Epoch 28: loss improved from 0.71533 to 0.69693, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 879ms/step - loss: 0.6969 - accuracy: 0.7109\n",
            "Epoch 29/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6670 - accuracy: 0.7139\n",
            "Epoch 29: loss improved from 0.69693 to 0.66703, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 872ms/step - loss: 0.6670 - accuracy: 0.7139\n",
            "Epoch 30/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.7318\n",
            "Epoch 30: loss improved from 0.66703 to 0.64346, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 865ms/step - loss: 0.6435 - accuracy: 0.7318\n",
            "Epoch 31/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6146 - accuracy: 0.7430\n",
            "Epoch 31: loss improved from 0.64346 to 0.61455, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 883ms/step - loss: 0.6146 - accuracy: 0.7430\n",
            "Epoch 32/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5774 - accuracy: 0.7617\n",
            "Epoch 32: loss improved from 0.61455 to 0.57738, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 95s 914ms/step - loss: 0.5774 - accuracy: 0.7617\n",
            "Epoch 33/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7678\n",
            "Epoch 33: loss improved from 0.57738 to 0.55451, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 870ms/step - loss: 0.5545 - accuracy: 0.7678\n",
            "Epoch 34/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5216 - accuracy: 0.7862\n",
            "Epoch 34: loss improved from 0.55451 to 0.52164, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 875ms/step - loss: 0.5216 - accuracy: 0.7862\n",
            "Epoch 35/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.7989\n",
            "Epoch 35: loss improved from 0.52164 to 0.49327, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 868ms/step - loss: 0.4933 - accuracy: 0.7989\n",
            "Epoch 36/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4951 - accuracy: 0.7956\n",
            "Epoch 36: loss did not improve from 0.49327\n",
            "104/104 [==============================] - 93s 888ms/step - loss: 0.4951 - accuracy: 0.7956\n",
            "Epoch 37/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4570 - accuracy: 0.8201\n",
            "Epoch 37: loss improved from 0.49327 to 0.45697, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 874ms/step - loss: 0.4570 - accuracy: 0.8201\n",
            "Epoch 38/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4235 - accuracy: 0.8292\n",
            "Epoch 38: loss improved from 0.45697 to 0.42355, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 871ms/step - loss: 0.4235 - accuracy: 0.8292\n",
            "Epoch 39/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8388\n",
            "Epoch 39: loss improved from 0.42355 to 0.40010, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 94s 903ms/step - loss: 0.4001 - accuracy: 0.8388\n",
            "Epoch 40/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4052 - accuracy: 0.8385\n",
            "Epoch 40: loss did not improve from 0.40010\n",
            "104/104 [==============================] - 91s 871ms/step - loss: 0.4052 - accuracy: 0.8385\n",
            "Epoch 41/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.3893 - accuracy: 0.8409\n",
            "Epoch 41: loss improved from 0.40010 to 0.38926, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 865ms/step - loss: 0.3893 - accuracy: 0.8409\n",
            "Epoch 42/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.3625 - accuracy: 0.8561\n",
            "Epoch 42: loss improved from 0.38926 to 0.36249, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 90s 869ms/step - loss: 0.3625 - accuracy: 0.8561\n",
            "Epoch 43/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.3411 - accuracy: 0.8654\n",
            "Epoch 43: loss improved from 0.36249 to 0.34109, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 91s 876ms/step - loss: 0.3411 - accuracy: 0.8654\n",
            "Epoch 44/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.8718\n",
            "Epoch 44: loss did not improve from 0.34109\n",
            "104/104 [==============================] - 90s 865ms/step - loss: 0.3423 - accuracy: 0.8718\n",
            "Epoch 45/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.8839\n",
            "Epoch 45: loss improved from 0.34109 to 0.30071, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 92s 887ms/step - loss: 0.3007 - accuracy: 0.8839\n",
            "Epoch 46/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.8848\n",
            "Epoch 46: loss improved from 0.30071 to 0.29761, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 96s 923ms/step - loss: 0.2976 - accuracy: 0.8848\n",
            "Epoch 47/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8969\n",
            "Epoch 47: loss improved from 0.29761 to 0.27592, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 93s 893ms/step - loss: 0.2759 - accuracy: 0.8969\n",
            "Epoch 48/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9035\n",
            "Epoch 48: loss improved from 0.27592 to 0.25097, saving model to best_model1.hdf5\n",
            "104/104 [==============================] - 95s 912ms/step - loss: 0.2510 - accuracy: 0.9035\n",
            "Epoch 49/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9087\n",
            "Epoch 49: loss did not improve from 0.25097\n",
            "104/104 [==============================] - 94s 902ms/step - loss: 0.2521 - accuracy: 0.9087\n",
            "Epoch 50/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.9111\n",
            "Epoch 50: loss did not improve from 0.25097\n",
            "104/104 [==============================] - 91s 872ms/step - loss: 0.2572 - accuracy: 0.9111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "Y_pred=model1.predict(X_test)\n",
        "Y_pred=np.argmax(Y_pred, axis=1)\n",
        "Y_test=np.argmax(Y_test, axis=1)\n",
        "print('\\nConfusion Matrix\\n')\n",
        "cm = confusion_matrix(Y_test, Y_pred)\n",
        "print(cm)\n",
        "\n",
        "acc1 = hist1.history['accuracy']\n",
        "print('\\nTraining Accuracy\\n')\n",
        "print('Mean Training Accuracy',np.mean(acc1))\n",
        "print('Max Training Accuracy',max(acc1))\n",
        "\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(Y_test, Y_pred, target_names= emotions_used))\n",
        "\n",
        "print('Class Wise Accuracy\\n')\n",
        "acc=0\n",
        "for idx in range(0,4):\n",
        "  sum=0\n",
        "  for j in range(0,4):\n",
        "    sum= sum + cm[idx][j]\n",
        "  acc = cm[idx][idx] / sum\n",
        "  print(emotions_used[idx],\" : \",acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O2AAkMfLR_M",
        "outputId": "a314c57a-0d20-44cf-e565-d15f989feced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 22s 431ms/step\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[127 142  90  15]\n",
            " [ 18 222  89  15]\n",
            " [ 14 101 321 115]\n",
            " [  4  18 106 232]]\n",
            "\n",
            "Training Accuracy\n",
            "\n",
            "Mean Training Accuracy 0.6830541229248047\n",
            "Max Training Accuracy 0.9110976457595825\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ang       0.78      0.34      0.47       374\n",
            "         exc       0.46      0.65      0.54       344\n",
            "         neu       0.53      0.58      0.55       551\n",
            "         sad       0.62      0.64      0.63       360\n",
            "\n",
            "    accuracy                           0.55      1629\n",
            "   macro avg       0.60      0.55      0.55      1629\n",
            "weighted avg       0.59      0.55      0.55      1629\n",
            "\n",
            "Class Wise Accuracy\n",
            "\n",
            "ang  :  0.339572192513369\n",
            "exc  :  0.6453488372093024\n",
            "neu  :  0.5825771324863884\n",
            "sad  :  0.6444444444444445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 2"
      ],
      "metadata": {
        "id": "I-pc_5aALBri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import LSTM,RNN\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "from tensorflow.keras.layers import InputSpec,Layer\n",
        "from tensorflow.keras.layers import LSTMCell\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "46wwmdl8LUbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def global_attention_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(256, return_sequences=True),input_shape=(20,2048)))\n",
        "    model.add(GlobalSelfAttention(num_heads=4,key_dim=128))\n",
        "    model.add(FeedForward(512,2048))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model2 = global_attention_model()\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAbpO7x5LUiq",
        "outputId": "c9030ffe-02d3-48cf-93d0-512aa51c7f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirectiona  (None, 20, 512)          4720640   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " global_self_attention (Glob  (None, 20, 512)          1051648   \n",
            " alSelfAttention)                                                \n",
            "                                                                 \n",
            " feed_forward (FeedForward)  (None, 20, 512)           2100736   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 10240)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                655424    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 64)               256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,528,964\n",
            "Trainable params: 8,528,836\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\"best_model2.hdf5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto',  save_freq='epoch')\n",
        "hist2 = model2.fit(X_train, Y_train, batch_size=32, epochs=50, verbose=1, shuffle = False,\n",
        "                 callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O3QZ9z9LUr6",
        "outputId": "30df32dc-c289-4d58-ae62-1d63145f347a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.2158 - accuracy: 0.4466\n",
            "Epoch 1: loss improved from inf to 1.21579, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 105s 901ms/step - loss: 1.2158 - accuracy: 0.4466\n",
            "Epoch 2/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0151 - accuracy: 0.5461\n",
            "Epoch 2: loss improved from 1.21579 to 1.01513, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 92s 884ms/step - loss: 1.0151 - accuracy: 0.5461\n",
            "Epoch 3/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9987 - accuracy: 0.5479\n",
            "Epoch 3: loss improved from 1.01513 to 0.99871, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 894ms/step - loss: 0.9987 - accuracy: 0.5479\n",
            "Epoch 4/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9424 - accuracy: 0.5894\n",
            "Epoch 4: loss improved from 0.99871 to 0.94245, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 92s 881ms/step - loss: 0.9424 - accuracy: 0.5894\n",
            "Epoch 5/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8669 - accuracy: 0.6178\n",
            "Epoch 5: loss improved from 0.94245 to 0.86688, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 92s 884ms/step - loss: 0.8669 - accuracy: 0.6178\n",
            "Epoch 6/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9030 - accuracy: 0.6060\n",
            "Epoch 6: loss did not improve from 0.86688\n",
            "104/104 [==============================] - 92s 882ms/step - loss: 0.9030 - accuracy: 0.6060\n",
            "Epoch 7/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9258 - accuracy: 0.5776\n",
            "Epoch 7: loss did not improve from 0.86688\n",
            "104/104 [==============================] - 91s 875ms/step - loss: 0.9258 - accuracy: 0.5776\n",
            "Epoch 8/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7526 - accuracy: 0.6807\n",
            "Epoch 8: loss improved from 0.86688 to 0.75255, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 889ms/step - loss: 0.7526 - accuracy: 0.6807\n",
            "Epoch 9/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.7155\n",
            "Epoch 9: loss improved from 0.75255 to 0.67590, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 94s 900ms/step - loss: 0.6759 - accuracy: 0.7155\n",
            "Epoch 10/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6036 - accuracy: 0.7472\n",
            "Epoch 10: loss improved from 0.67590 to 0.60364, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 893ms/step - loss: 0.6036 - accuracy: 0.7472\n",
            "Epoch 11/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.7862\n",
            "Epoch 11: loss improved from 0.60364 to 0.53888, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 94s 904ms/step - loss: 0.5389 - accuracy: 0.7862\n",
            "Epoch 12/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4707 - accuracy: 0.8174\n",
            "Epoch 12: loss improved from 0.53888 to 0.47066, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 92s 880ms/step - loss: 0.4707 - accuracy: 0.8174\n",
            "Epoch 13/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.8198\n",
            "Epoch 13: loss improved from 0.47066 to 0.43874, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 91s 878ms/step - loss: 0.4387 - accuracy: 0.8198\n",
            "Epoch 14/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.8576\n",
            "Epoch 14: loss improved from 0.43874 to 0.36288, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 91s 881ms/step - loss: 0.3629 - accuracy: 0.8576\n",
            "Epoch 15/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.8555\n",
            "Epoch 15: loss did not improve from 0.36288\n",
            "104/104 [==============================] - 91s 880ms/step - loss: 0.3792 - accuracy: 0.8555\n",
            "Epoch 16/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.8809\n",
            "Epoch 16: loss improved from 0.36288 to 0.31212, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 91s 872ms/step - loss: 0.3121 - accuracy: 0.8809\n",
            "Epoch 17/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.8996\n",
            "Epoch 17: loss improved from 0.31212 to 0.27551, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 887ms/step - loss: 0.2755 - accuracy: 0.8996\n",
            "Epoch 18/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9311\n",
            "Epoch 18: loss improved from 0.27551 to 0.20280, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 91s 874ms/step - loss: 0.2028 - accuracy: 0.9311\n",
            "Epoch 19/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.2233 - accuracy: 0.9199\n",
            "Epoch 19: loss did not improve from 0.20280\n",
            "104/104 [==============================] - 91s 880ms/step - loss: 0.2233 - accuracy: 0.9199\n",
            "Epoch 20/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9341\n",
            "Epoch 20: loss improved from 0.20280 to 0.19704, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 92s 885ms/step - loss: 0.1970 - accuracy: 0.9341\n",
            "Epoch 21/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9592\n",
            "Epoch 21: loss improved from 0.19704 to 0.13110, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 92s 887ms/step - loss: 0.1311 - accuracy: 0.9592\n",
            "Epoch 22/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9670\n",
            "Epoch 22: loss improved from 0.13110 to 0.11640, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 94s 904ms/step - loss: 0.1164 - accuracy: 0.9670\n",
            "Epoch 23/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9743\n",
            "Epoch 23: loss improved from 0.11640 to 0.09395, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 900ms/step - loss: 0.0940 - accuracy: 0.9743\n",
            "Epoch 24/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9770\n",
            "Epoch 24: loss improved from 0.09395 to 0.08581, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 101s 967ms/step - loss: 0.0858 - accuracy: 0.9770\n",
            "Epoch 25/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9773\n",
            "Epoch 25: loss improved from 0.08581 to 0.08377, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 97s 930ms/step - loss: 0.0838 - accuracy: 0.9773\n",
            "Epoch 26/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9822\n",
            "Epoch 26: loss improved from 0.08377 to 0.06601, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 96s 918ms/step - loss: 0.0660 - accuracy: 0.9822\n",
            "Epoch 27/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9876\n",
            "Epoch 27: loss improved from 0.06601 to 0.05558, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 97s 930ms/step - loss: 0.0556 - accuracy: 0.9876\n",
            "Epoch 28/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9852\n",
            "Epoch 28: loss did not improve from 0.05558\n",
            "104/104 [==============================] - 94s 907ms/step - loss: 0.0562 - accuracy: 0.9852\n",
            "Epoch 29/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9794\n",
            "Epoch 29: loss did not improve from 0.05558\n",
            "104/104 [==============================] - 94s 900ms/step - loss: 0.0649 - accuracy: 0.9794\n",
            "Epoch 30/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9921\n",
            "Epoch 30: loss improved from 0.05558 to 0.03825, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 899ms/step - loss: 0.0383 - accuracy: 0.9921\n",
            "Epoch 31/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9943\n",
            "Epoch 31: loss improved from 0.03825 to 0.03372, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 95s 911ms/step - loss: 0.0337 - accuracy: 0.9943\n",
            "Epoch 32/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9930\n",
            "Epoch 32: loss improved from 0.03372 to 0.03210, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 892ms/step - loss: 0.0321 - accuracy: 0.9930\n",
            "Epoch 33/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9940\n",
            "Epoch 33: loss improved from 0.03210 to 0.02572, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 891ms/step - loss: 0.0257 - accuracy: 0.9940\n",
            "Epoch 34/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9952\n",
            "Epoch 34: loss did not improve from 0.02572\n",
            "104/104 [==============================] - 94s 903ms/step - loss: 0.0268 - accuracy: 0.9952\n",
            "Epoch 35/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9946\n",
            "Epoch 35: loss improved from 0.02572 to 0.02406, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 92s 885ms/step - loss: 0.0241 - accuracy: 0.9946\n",
            "Epoch 36/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9985\n",
            "Epoch 36: loss improved from 0.02406 to 0.01316, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 91s 880ms/step - loss: 0.0132 - accuracy: 0.9985\n",
            "Epoch 37/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9994\n",
            "Epoch 37: loss improved from 0.01316 to 0.00647, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 891ms/step - loss: 0.0065 - accuracy: 0.9994\n",
            "Epoch 38/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 38: loss improved from 0.00647 to 0.00280, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 94s 909ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 39: loss improved from 0.00280 to 0.00173, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 893ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 40: loss improved from 0.00173 to 0.00139, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 94s 909ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 41: loss improved from 0.00139 to 0.00115, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 95s 910ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 9.8531e-04 - accuracy: 1.0000\n",
            "Epoch 42: loss improved from 0.00115 to 0.00099, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 92s 888ms/step - loss: 9.8531e-04 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 8.6032e-04 - accuracy: 1.0000\n",
            "Epoch 43: loss improved from 0.00099 to 0.00086, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 95s 913ms/step - loss: 8.6032e-04 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 7.3192e-04 - accuracy: 1.0000\n",
            "Epoch 44: loss improved from 0.00086 to 0.00073, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 97s 930ms/step - loss: 7.3192e-04 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 6.3276e-04 - accuracy: 1.0000\n",
            "Epoch 45: loss improved from 0.00073 to 0.00063, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 92s 883ms/step - loss: 6.3276e-04 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 5.6610e-04 - accuracy: 1.0000\n",
            "Epoch 46: loss improved from 0.00063 to 0.00057, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 894ms/step - loss: 5.6610e-04 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 5.0232e-04 - accuracy: 1.0000\n",
            "Epoch 47: loss improved from 0.00057 to 0.00050, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 95s 916ms/step - loss: 5.0232e-04 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 4.4525e-04 - accuracy: 1.0000\n",
            "Epoch 48: loss improved from 0.00050 to 0.00045, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 93s 898ms/step - loss: 4.4525e-04 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 3.8924e-04 - accuracy: 1.0000\n",
            "Epoch 49: loss improved from 0.00045 to 0.00039, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 94s 902ms/step - loss: 3.8924e-04 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 3.4453e-04 - accuracy: 1.0000\n",
            "Epoch 50: loss improved from 0.00039 to 0.00034, saving model to best_model2.hdf5\n",
            "104/104 [==============================] - 95s 918ms/step - loss: 3.4453e-04 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "Y_pred=model2.predict(X_test)\n",
        "Y_pred=np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('\\nConfusion Matrix\\n')\n",
        "\n",
        "cm = confusion_matrix(Y_test, Y_pred)\n",
        "print(cm)\n",
        "print('\\nTraining Accuracy\\n',)\n",
        "acc2 = hist2.history['accuracy']\n",
        "print('Mean Training Accuracy',np.mean(acc2))\n",
        "print('Max Training Accuracy',max(acc2))\n",
        "\n",
        "\n",
        "\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(Y_test, Y_pred, target_names= emotions_used))\n",
        "\n",
        "\n",
        "# Calculate the accuracy for each one of our classes\n",
        "print('Class Wise Accuracy\\n')\n",
        "acc=0\n",
        "for idx in range(0,4):\n",
        "  sum=0\n",
        "  for j in range(0,4):\n",
        "    sum= sum + cm[idx][j]\n",
        "  acc = cm[idx][idx] / sum\n",
        "  print(emotions_used[idx],\" : \",acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cifaRk5TLfde",
        "outputId": "e30ca1bb-f0f9-45af-ab7d-3d364995ab22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 20s 387ms/step\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[233  62  53  26]\n",
            " [ 81 162  84  17]\n",
            " [ 72  83 295 101]\n",
            " [ 21   8  91 240]]\n",
            "\n",
            "Training Accuracy\n",
            "\n",
            "Mean Training Accuracy 0.8905352252721787\n",
            "Max Training Accuracy 1.0\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ang       0.57      0.62      0.60       374\n",
            "         exc       0.51      0.47      0.49       344\n",
            "         neu       0.56      0.54      0.55       551\n",
            "         sad       0.62      0.67      0.65       360\n",
            "\n",
            "    accuracy                           0.57      1629\n",
            "   macro avg       0.57      0.57      0.57      1629\n",
            "weighted avg       0.57      0.57      0.57      1629\n",
            "\n",
            "Class Wise Accuracy\n",
            "\n",
            "ang  :  0.6229946524064172\n",
            "exc  :  0.47093023255813954\n",
            "neu  :  0.5353901996370236\n",
            "sad  :  0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 3"
      ],
      "metadata": {
        "id": "UrJfyi7IK8NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import LSTM,RNN\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "from tensorflow.keras.layers import InputSpec,Layer\n",
        "from tensorflow.keras.layers import LSTMCell\n",
        "\n",
        "# Add attention layer to the deep learning network\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),\n",
        "                               initializer='random_normal', trainable=True)\n",
        "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),\n",
        "                               initializer='zeros', trainable=True)\n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        # Alignment scores. Pass them through tanh function\n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        # Remove dimension of size 1\n",
        "        e = K.squeeze(e, axis=-1)\n",
        "        # Compute the weights\n",
        "        alpha = K.softmax(e)\n",
        "        # Reshape to tensorFlow format\n",
        "        alpha = K.expand_dims(alpha, axis=-1)\n",
        "        # Compute the context vector\n",
        "        context = x * alpha\n",
        "        context = K.sum(context, axis=1)\n",
        "        return context\n"
      ],
      "metadata": {
        "id": "5ErPDdSfM-qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, SimpleRNN, LSTM, Bidirectional\n",
        "def LSTMattention(hidden_units, dense_units, input_shape, activation):\n",
        "    mainmodel=Sequential()\n",
        "    x=Input(shape=input_shape)\n",
        "    BiLSTM_layer = Bidirectional(LSTM(hidden_units, return_sequences=True))(x)\n",
        "    attention_layer = attention()(BiLSTM_layer)\n",
        "    outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n",
        "    model=Model(x,outputs)\n",
        "    mainmodel.add(model)\n",
        "    mainmodel.add(Dense(512, activation='relu'))\n",
        "    mainmodel.add(Dense(4, activation='softmax'))\n",
        "\n",
        "    mainmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "    return mainmodel\n",
        "\n",
        "model3 = LSTMattention(hidden_units=128, dense_units=4,input_shape=(20,2048), activation='tanh')\n",
        "model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN3_nWrwNK18",
        "outputId": "84b6b1df-6c03-4cda-d2e9-93bce28119f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (Functional)          (None, 4)                 2230552   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 512)               2560      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,235,164\n",
            "Trainable params: 2,235,164\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\"best_model3.hdf5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto',  save_freq='epoch')\n",
        "hist3 = model3.fit(X_train, Y_train, batch_size=32, epochs=50, verbose=1, shuffle = False,\n",
        "                 callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07D93FYsNOcz",
        "outputId": "6c7217b2-33bb-4e87-ba3a-ceb550585c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.2685 - accuracy: 0.3973\n",
            "Epoch 1: loss improved from inf to 1.26849, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 34s 274ms/step - loss: 1.2685 - accuracy: 0.3973\n",
            "Epoch 2/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.1330 - accuracy: 0.4805\n",
            "Epoch 2: loss improved from 1.26849 to 1.13302, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 271ms/step - loss: 1.1330 - accuracy: 0.4805\n",
            "Epoch 3/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0904 - accuracy: 0.4977\n",
            "Epoch 3: loss improved from 1.13302 to 1.09038, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 271ms/step - loss: 1.0904 - accuracy: 0.4977\n",
            "Epoch 4/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0633 - accuracy: 0.5116\n",
            "Epoch 4: loss improved from 1.09038 to 1.06332, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 272ms/step - loss: 1.0633 - accuracy: 0.5116\n",
            "Epoch 5/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0349 - accuracy: 0.5265\n",
            "Epoch 5: loss improved from 1.06332 to 1.03485, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 269ms/step - loss: 1.0349 - accuracy: 0.5265\n",
            "Epoch 6/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 1.0098 - accuracy: 0.5467\n",
            "Epoch 6: loss improved from 1.03485 to 1.00978, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 268ms/step - loss: 1.0098 - accuracy: 0.5467\n",
            "Epoch 7/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9960 - accuracy: 0.5525\n",
            "Epoch 7: loss improved from 1.00978 to 0.99601, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 260ms/step - loss: 0.9960 - accuracy: 0.5525\n",
            "Epoch 8/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9810 - accuracy: 0.5606\n",
            "Epoch 8: loss improved from 0.99601 to 0.98097, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 259ms/step - loss: 0.9810 - accuracy: 0.5606\n",
            "Epoch 9/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9645 - accuracy: 0.5709\n",
            "Epoch 9: loss improved from 0.98097 to 0.96447, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 269ms/step - loss: 0.9645 - accuracy: 0.5709\n",
            "Epoch 10/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9477 - accuracy: 0.5757\n",
            "Epoch 10: loss improved from 0.96447 to 0.94773, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 268ms/step - loss: 0.9477 - accuracy: 0.5757\n",
            "Epoch 11/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9377 - accuracy: 0.5842\n",
            "Epoch 11: loss improved from 0.94773 to 0.93769, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 268ms/step - loss: 0.9377 - accuracy: 0.5842\n",
            "Epoch 12/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9267 - accuracy: 0.5894\n",
            "Epoch 12: loss improved from 0.93769 to 0.92673, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 29s 282ms/step - loss: 0.9267 - accuracy: 0.5894\n",
            "Epoch 13/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.9106 - accuracy: 0.5990\n",
            "Epoch 13: loss improved from 0.92673 to 0.91065, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 268ms/step - loss: 0.9106 - accuracy: 0.5990\n",
            "Epoch 14/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8963 - accuracy: 0.6105\n",
            "Epoch 14: loss improved from 0.91065 to 0.89626, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 26s 252ms/step - loss: 0.8963 - accuracy: 0.6105\n",
            "Epoch 15/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8820 - accuracy: 0.6169\n",
            "Epoch 15: loss improved from 0.89626 to 0.88204, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 26s 248ms/step - loss: 0.8820 - accuracy: 0.6169\n",
            "Epoch 16/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8705 - accuracy: 0.6232\n",
            "Epoch 16: loss improved from 0.88204 to 0.87051, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 259ms/step - loss: 0.8705 - accuracy: 0.6232\n",
            "Epoch 17/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8579 - accuracy: 0.6329\n",
            "Epoch 17: loss improved from 0.87051 to 0.85786, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 256ms/step - loss: 0.8579 - accuracy: 0.6329\n",
            "Epoch 18/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8387 - accuracy: 0.6399\n",
            "Epoch 18: loss improved from 0.85786 to 0.83869, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 260ms/step - loss: 0.8387 - accuracy: 0.6399\n",
            "Epoch 19/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8267 - accuracy: 0.6474\n",
            "Epoch 19: loss improved from 0.83869 to 0.82670, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 25s 241ms/step - loss: 0.8267 - accuracy: 0.6474\n",
            "Epoch 20/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.8081 - accuracy: 0.6547\n",
            "Epoch 20: loss improved from 0.82670 to 0.80814, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 259ms/step - loss: 0.8081 - accuracy: 0.6547\n",
            "Epoch 21/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7889 - accuracy: 0.6616\n",
            "Epoch 21: loss improved from 0.80814 to 0.78888, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 260ms/step - loss: 0.7889 - accuracy: 0.6616\n",
            "Epoch 22/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7799 - accuracy: 0.6749\n",
            "Epoch 22: loss improved from 0.78888 to 0.77994, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 261ms/step - loss: 0.7799 - accuracy: 0.6749\n",
            "Epoch 23/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7684 - accuracy: 0.6807\n",
            "Epoch 23: loss improved from 0.77994 to 0.76840, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 261ms/step - loss: 0.7684 - accuracy: 0.6807\n",
            "Epoch 24/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7466 - accuracy: 0.6904\n",
            "Epoch 24: loss improved from 0.76840 to 0.74659, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 25s 243ms/step - loss: 0.7466 - accuracy: 0.6904\n",
            "Epoch 25/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.7037\n",
            "Epoch 25: loss improved from 0.74659 to 0.72262, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 26s 253ms/step - loss: 0.7226 - accuracy: 0.7037\n",
            "Epoch 26/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6994 - accuracy: 0.7148\n",
            "Epoch 26: loss improved from 0.72262 to 0.69944, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 262ms/step - loss: 0.6994 - accuracy: 0.7148\n",
            "Epoch 27/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6788 - accuracy: 0.7233\n",
            "Epoch 27: loss improved from 0.69944 to 0.67877, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 262ms/step - loss: 0.6788 - accuracy: 0.7233\n",
            "Epoch 28/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6440 - accuracy: 0.7430\n",
            "Epoch 28: loss improved from 0.67877 to 0.64403, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 266ms/step - loss: 0.6440 - accuracy: 0.7430\n",
            "Epoch 29/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6178 - accuracy: 0.7581\n",
            "Epoch 29: loss improved from 0.64403 to 0.61775, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 263ms/step - loss: 0.6178 - accuracy: 0.7581\n",
            "Epoch 30/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5949 - accuracy: 0.7693\n",
            "Epoch 30: loss improved from 0.61775 to 0.59489, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 26s 253ms/step - loss: 0.5949 - accuracy: 0.7693\n",
            "Epoch 31/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.7780\n",
            "Epoch 31: loss improved from 0.59489 to 0.57051, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 265ms/step - loss: 0.5705 - accuracy: 0.7780\n",
            "Epoch 32/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5694 - accuracy: 0.7777\n",
            "Epoch 32: loss improved from 0.57051 to 0.56937, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 267ms/step - loss: 0.5694 - accuracy: 0.7777\n",
            "Epoch 33/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6052 - accuracy: 0.7539\n",
            "Epoch 33: loss did not improve from 0.56937\n",
            "104/104 [==============================] - 28s 267ms/step - loss: 0.6052 - accuracy: 0.7539\n",
            "Epoch 34/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6277 - accuracy: 0.7490\n",
            "Epoch 34: loss did not improve from 0.56937\n",
            "104/104 [==============================] - 28s 267ms/step - loss: 0.6277 - accuracy: 0.7490\n",
            "Epoch 35/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.7396\n",
            "Epoch 35: loss did not improve from 0.56937\n",
            "104/104 [==============================] - 27s 264ms/step - loss: 0.6256 - accuracy: 0.7396\n",
            "Epoch 36/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5864 - accuracy: 0.7605\n",
            "Epoch 36: loss did not improve from 0.56937\n",
            "104/104 [==============================] - 26s 250ms/step - loss: 0.5864 - accuracy: 0.7605\n",
            "Epoch 37/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5626 - accuracy: 0.7729\n",
            "Epoch 37: loss improved from 0.56937 to 0.56262, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 27s 261ms/step - loss: 0.5626 - accuracy: 0.7729\n",
            "Epoch 38/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.7817\n",
            "Epoch 38: loss improved from 0.56262 to 0.54222, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 270ms/step - loss: 0.5422 - accuracy: 0.7817\n",
            "Epoch 39/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5167 - accuracy: 0.7953\n",
            "Epoch 39: loss improved from 0.54222 to 0.51672, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 270ms/step - loss: 0.5167 - accuracy: 0.7953\n",
            "Epoch 40/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5006 - accuracy: 0.8037\n",
            "Epoch 40: loss improved from 0.51672 to 0.50060, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 265ms/step - loss: 0.5006 - accuracy: 0.8037\n",
            "Epoch 41/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4823 - accuracy: 0.8077\n",
            "Epoch 41: loss improved from 0.50060 to 0.48229, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 265ms/step - loss: 0.4823 - accuracy: 0.8077\n",
            "Epoch 42/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4827 - accuracy: 0.8083\n",
            "Epoch 42: loss did not improve from 0.48229\n",
            "104/104 [==============================] - 27s 262ms/step - loss: 0.4827 - accuracy: 0.8083\n",
            "Epoch 43/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4829 - accuracy: 0.8101\n",
            "Epoch 43: loss did not improve from 0.48229\n",
            "104/104 [==============================] - 26s 250ms/step - loss: 0.4829 - accuracy: 0.8101\n",
            "Epoch 44/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4863 - accuracy: 0.8041\n",
            "Epoch 44: loss did not improve from 0.48229\n",
            "104/104 [==============================] - 28s 264ms/step - loss: 0.4863 - accuracy: 0.8041\n",
            "Epoch 45/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.5369 - accuracy: 0.7850\n",
            "Epoch 45: loss did not improve from 0.48229\n",
            "104/104 [==============================] - 28s 267ms/step - loss: 0.5369 - accuracy: 0.7850\n",
            "Epoch 46/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4746 - accuracy: 0.8125\n",
            "Epoch 46: loss improved from 0.48229 to 0.47457, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 267ms/step - loss: 0.4746 - accuracy: 0.8125\n",
            "Epoch 47/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.8183\n",
            "Epoch 47: loss improved from 0.47457 to 0.45212, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 36s 348ms/step - loss: 0.4521 - accuracy: 0.8183\n",
            "Epoch 48/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4496 - accuracy: 0.8174\n",
            "Epoch 48: loss improved from 0.45212 to 0.44963, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 29s 274ms/step - loss: 0.4496 - accuracy: 0.8174\n",
            "Epoch 49/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4625 - accuracy: 0.8161\n",
            "Epoch 49: loss did not improve from 0.44963\n",
            "104/104 [==============================] - 28s 271ms/step - loss: 0.4625 - accuracy: 0.8161\n",
            "Epoch 50/50\n",
            "104/104 [==============================] - ETA: 0s - loss: 0.4354 - accuracy: 0.8304\n",
            "Epoch 50: loss improved from 0.44963 to 0.43538, saving model to best_model3.hdf5\n",
            "104/104 [==============================] - 28s 267ms/step - loss: 0.4354 - accuracy: 0.8304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "Y_pred=model3.predict(X_test)\n",
        "Y_pred=np.argmax(Y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(Y_test, Y_pred)\n",
        "print('\\nConfusion Matrix\\n')\n",
        "print(cm)\n",
        "\n",
        "acc3 = hist3.history['accuracy']\n",
        "print('\\nTraining Accuracy\\n',)\n",
        "print('Mean Training Accuracy',np.mean(acc3))\n",
        "print('Max Training Accuracy',max(acc3))\n",
        "\n",
        "print('\\nClassification Report\\n')\n",
        "print(classification_report(Y_test, Y_pred, target_names= emotions_used))\n",
        "\n",
        "# Calculate the accuracy for each one of our classes\n",
        "\n",
        "print('Class Wise Accuracy\\n')\n",
        "acc=0\n",
        "for idx in range(0,4):\n",
        "  sum=0\n",
        "  for j in range(0,4):\n",
        "    sum= sum + cm[idx][j]\n",
        "  acc = cm[idx][idx] / sum\n",
        "  print(emotions_used[idx],\" : \",acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klXUMt8MNbW6",
        "outputId": "4c435b3e-f4fc-4d9b-b4f9-31a45130d7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 9s 171ms/step\n",
            "\n",
            "Confusion Matrix\n",
            "\n",
            "[[219  81  47  27]\n",
            " [ 55 203  66  20]\n",
            " [ 54  78 310 109]\n",
            " [ 13  10  59 278]]\n",
            "\n",
            "Training Accuracy\n",
            "\n",
            "Mean Training Accuracy 0.6872028976678848\n",
            "Max Training Accuracy 0.8303598165512085\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ang       0.64      0.59      0.61       374\n",
            "         exc       0.55      0.59      0.57       344\n",
            "         neu       0.64      0.56      0.60       551\n",
            "         sad       0.64      0.77      0.70       360\n",
            "\n",
            "    accuracy                           0.62      1629\n",
            "   macro avg       0.62      0.63      0.62      1629\n",
            "weighted avg       0.62      0.62      0.62      1629\n",
            "\n",
            "Class Wise Accuracy\n",
            "\n",
            "ang  :  0.5855614973262032\n",
            "exc  :  0.5901162790697675\n",
            "neu  :  0.5626134301270418\n",
            "sad  :  0.7722222222222223\n"
          ]
        }
      ]
    }
  ]
}